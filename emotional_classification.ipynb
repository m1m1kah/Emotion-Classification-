{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOx3/FZoicySa4PPJCjnwTU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"-Myb_qwKiyB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Emotional Classification CNN project\n","\n","This project is to help me learn about how CNN's work and how do i implement one - from a collection of images to an actual working architecture.\n","\n","The dataset is from Kaggle and is of a bunch of different faces grouped into different emotions:\n","  - angry\n","  - disgusted\n","  - fearful\n","  - happy\n","  - neutral\n","  - sad\n","  - surprised\n","\n","\n","Initially I used VScode as my IDE however i didn't have good GPU support so i chose Colab instead."],"metadata":{"id":"CQZVHdhtsx4B"}},{"cell_type":"markdown","source":["### 1. Upload files to Colab"],"metadata":{"id":"YBeyeWSotogV"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"CT8okyRfgFtf","executionInfo":{"status":"ok","timestamp":1749638229760,"user_tz":-60,"elapsed":361142,"user":{"displayName":"maiesha siddika","userId":"15963573170624768236"}},"outputId":"cb398316-fca7-4d9b-e3a8-1877c4550e60"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e3ddc001-d7a4-442d-b008-d6d7002241d6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e3ddc001-d7a4-442d-b008-d6d7002241d6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving faces.zip to faces.zip\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # This will open a file picker to upload files"]},{"cell_type":"code","source":["!unzip -q faces.zip"],"metadata":{"id":"8drkUbv0hrhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Import required modules"],"metadata":{"id":"EE6zfnkCtrls"}},{"cell_type":"code","source":["import pandas as pd\n","import torch                       # core PyTorch library\n","import torch.nn as nn              # neural network modules (layers, loss functions)\n","import torch.nn.functional as F\n","import torch.optim as optim        # optimizers like SGD, Adam\n","from torch.utils.data import DataLoader  # for batching and shuffling data\n","from torchvision import datasets, transforms  # popular vision datasets and image transforms\n","import matplotlib.pyplot as plt    # to plot loss, accuracy, images\n","import numpy as np                 # numerical operations (optional but handy)\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n","from torch.utils.data import Subset\n","from tqdm import tqdm"],"metadata":{"id":"Dxv4XFVJh6qv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Converts images into necessary format and then into tensors, then uses Dataloaders\n"],"metadata":{"id":"KxlADYPBu0Gs"}},{"cell_type":"code","source":["print(\"Script started\")\n","\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),        # Converts images to grayscale (1 channel).\n","    transforms.Resize((64, 64)),                         # Resizes all images to 64x64 pixels.\n","    transforms.RandomHorizontalFlip(),                   # Randomly flip images horizontally (for data augmentation).\n","    transforms.RandomRotation(10),                       # Randomly rotate images by up to ±10 degrees.\n","    transforms.ToTensor(),                               # Convert images to PyTorch tensors (shape: [C, H, W]).\n","    transforms.Normalize((0.5,), (0.5,))                  # Normalize tensor values: mean=0.5, std=0.5.\n","])\n","\n","train_dataset = ImageFolder(root='faces/train', transform=transform) # Loads the training data from folders\n","test_dataset = ImageFolder(root='faces/test', transform=transform)\n","\n","#train_dataset_small = Subset(train_dataset, range(100))\n","#train_loader = DataLoader(train_dataset_small, batch_size=32, shuffle=True, num_workers=0)\n","# above code: uses only the first 100 training samples, useful for debugging or fast testing\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Wraps datasets into DataLoaders for batching and shuffling\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","images, labels = next(iter(train_loader))\n","print(images.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hg26XKQaiQIm","executionInfo":{"status":"ok","timestamp":1749638734403,"user_tz":-60,"elapsed":95,"user":{"displayName":"maiesha siddika","userId":"15963573170624768236"}},"outputId":"3dd8e7f8-98a8-4e3e-8d61-0242a3cfc17c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Script started\n","torch.Size([32, 1, 64, 64])\n"]}]},{"cell_type":"markdown","source":["### 4. CNN architecture"],"metadata":{"id":"vZci-xD8vBiG"}},{"cell_type":"code","source":["# Define the CNN model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.25)\n","        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n","        self.fc2 = nn.Linear(256, 7)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n","        x = x.view(-1, 128 * 8 * 8)\n","        x = self.dropout(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"-rr-ojstiVvj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Training loop"],"metadata":{"id":"t96g0D1evHfk"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model = CNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(50):\n","    running_loss = 0.0\n","    model.train()\n","    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","    for images, labels in loop:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        loop.set_postfix(loss=running_loss / len(train_loader))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPYPzG6WkVTF","executionInfo":{"status":"ok","timestamp":1749641347079,"user_tz":-60,"elapsed":1370218,"user":{"displayName":"maiesha siddika","userId":"15963573170624768236"}},"outputId":"cbc766fa-ab4a-4308-ec2b-e392c317e3cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 898/898 [00:28<00:00, 31.25it/s, loss=1.63]\n","Epoch 2: 100%|██████████| 898/898 [00:27<00:00, 32.67it/s, loss=1.38]\n","Epoch 3: 100%|██████████| 898/898 [00:27<00:00, 32.20it/s, loss=1.3]\n","Epoch 4: 100%|██████████| 898/898 [00:27<00:00, 32.66it/s, loss=1.25]\n","Epoch 5: 100%|██████████| 898/898 [00:27<00:00, 32.10it/s, loss=1.21]\n","Epoch 6: 100%|██████████| 898/898 [00:27<00:00, 32.93it/s, loss=1.18]\n","Epoch 7: 100%|██████████| 898/898 [00:27<00:00, 32.51it/s, loss=1.16]\n","Epoch 8: 100%|██████████| 898/898 [00:27<00:00, 32.74it/s, loss=1.13]\n","Epoch 9: 100%|██████████| 898/898 [00:27<00:00, 32.15it/s, loss=1.11]\n","Epoch 10: 100%|██████████| 898/898 [00:27<00:00, 32.86it/s, loss=1.09]\n","Epoch 11: 100%|██████████| 898/898 [00:27<00:00, 32.87it/s, loss=1.07]\n","Epoch 12: 100%|██████████| 898/898 [00:27<00:00, 32.85it/s, loss=1.05]\n","Epoch 13: 100%|██████████| 898/898 [00:27<00:00, 32.69it/s, loss=1.04]\n","Epoch 14: 100%|██████████| 898/898 [00:27<00:00, 32.64it/s, loss=1.03]\n","Epoch 15: 100%|██████████| 898/898 [00:27<00:00, 32.95it/s, loss=1.01]\n","Epoch 16: 100%|██████████| 898/898 [00:27<00:00, 32.82it/s, loss=0.996]\n","Epoch 17: 100%|██████████| 898/898 [00:27<00:00, 32.64it/s, loss=0.994]\n","Epoch 18: 100%|██████████| 898/898 [00:27<00:00, 32.28it/s, loss=0.973]\n","Epoch 19: 100%|██████████| 898/898 [00:26<00:00, 33.64it/s, loss=0.968]\n","Epoch 20: 100%|██████████| 898/898 [00:26<00:00, 33.86it/s, loss=0.961]\n","Epoch 21: 100%|██████████| 898/898 [00:26<00:00, 33.49it/s, loss=0.951]\n","Epoch 22: 100%|██████████| 898/898 [00:26<00:00, 33.67it/s, loss=0.941]\n","Epoch 23: 100%|██████████| 898/898 [00:26<00:00, 33.46it/s, loss=0.934]\n","Epoch 24: 100%|██████████| 898/898 [00:26<00:00, 34.05it/s, loss=0.928]\n","Epoch 25: 100%|██████████| 898/898 [00:26<00:00, 33.42it/s, loss=0.919]\n","Epoch 26: 100%|██████████| 898/898 [00:27<00:00, 32.08it/s, loss=0.902]\n","Epoch 27: 100%|██████████| 898/898 [00:28<00:00, 31.91it/s, loss=0.902]\n","Epoch 28: 100%|██████████| 898/898 [00:28<00:00, 31.81it/s, loss=0.897]\n","Epoch 29: 100%|██████████| 898/898 [00:27<00:00, 32.65it/s, loss=0.896]\n","Epoch 30: 100%|██████████| 898/898 [00:27<00:00, 33.20it/s, loss=0.88]\n","Epoch 31: 100%|██████████| 898/898 [00:28<00:00, 31.86it/s, loss=0.872]\n","Epoch 32: 100%|██████████| 898/898 [00:28<00:00, 32.03it/s, loss=0.871]\n","Epoch 33: 100%|██████████| 898/898 [00:27<00:00, 33.01it/s, loss=0.86]\n","Epoch 34: 100%|██████████| 898/898 [00:27<00:00, 32.93it/s, loss=0.86]\n","Epoch 35: 100%|██████████| 898/898 [00:27<00:00, 32.24it/s, loss=0.858]\n","Epoch 36: 100%|██████████| 898/898 [00:27<00:00, 33.14it/s, loss=0.847]\n","Epoch 37: 100%|██████████| 898/898 [00:27<00:00, 32.79it/s, loss=0.843]\n","Epoch 38: 100%|██████████| 898/898 [00:27<00:00, 33.11it/s, loss=0.829]\n","Epoch 39: 100%|██████████| 898/898 [00:27<00:00, 32.98it/s, loss=0.83]\n","Epoch 40: 100%|██████████| 898/898 [00:27<00:00, 33.23it/s, loss=0.824]\n","Epoch 41: 100%|██████████| 898/898 [00:27<00:00, 33.14it/s, loss=0.822]\n","Epoch 42: 100%|██████████| 898/898 [00:27<00:00, 33.13it/s, loss=0.816]\n","Epoch 43: 100%|██████████| 898/898 [00:27<00:00, 32.84it/s, loss=0.809]\n","Epoch 44: 100%|██████████| 898/898 [00:28<00:00, 31.65it/s, loss=0.803]\n","Epoch 45: 100%|██████████| 898/898 [00:27<00:00, 32.86it/s, loss=0.807]\n","Epoch 46: 100%|██████████| 898/898 [00:27<00:00, 32.66it/s, loss=0.796]\n","Epoch 47: 100%|██████████| 898/898 [00:26<00:00, 33.27it/s, loss=0.789]\n","Epoch 48: 100%|██████████| 898/898 [00:27<00:00, 33.04it/s, loss=0.785]\n","Epoch 49: 100%|██████████| 898/898 [00:27<00:00, 33.01it/s, loss=0.782]\n","Epoch 50: 100%|██████████| 898/898 [00:27<00:00, 33.13it/s, loss=0.776]\n"]}]},{"cell_type":"markdown","source":["### 6. Evaluation step"],"metadata":{"id":"qRnJL_XtvPJ_"}},{"cell_type":"code","source":["model.eval()\n","correct, total = 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30-wT__LkWKu","executionInfo":{"status":"ok","timestamp":1749641493858,"user_tz":-60,"elapsed":5052,"user":{"displayName":"maiesha siddika","userId":"15963573170624768236"}},"outputId":"6203aa4c-69ee-4075-f82a-1e89bfe15c05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 61.20%\n"]}]},{"cell_type":"markdown","source":["## Conclusion\n","\n","The Test accuracy is 61.2%. To improve the model, here are some changes I would make to the code:\n","  - Have more transformations on the images so model has more data to work from\n","  - play around with the number of epochs\n","  - play around with learning rate\n","  - early stopping?\n","  - Regularization techniques ?"],"metadata":{"id":"6qnHsgCpva6F"}}]}